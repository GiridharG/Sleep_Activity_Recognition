{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deee89ca-ce0d-49f5-ae00-bc6e3c3e7724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace_tools in d:\\anaconda\\lib\\site-packages (0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ace_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6561a03f-f8e2-4d03-bac7-9cd8c6852ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset generation function\n",
    "def generate_large_synthetic_data(samples_per_stage=150):\n",
    "    stages = [\"Awake\", \"Light\", \"Deep\", \"REM\"]\n",
    "    data = []\n",
    "\n",
    "    for stage in stages:\n",
    "        # Generate synthetic heart rate (HR) data\n",
    "        hr = np.random.normal(70 if stage == \"Awake\" else 60 if stage == \"Light\" else 50 if stage == \"Deep\" else 65, 10, samples_per_stage)\n",
    "        \n",
    "        # Generate synthetic accelerometer (ACC) data\n",
    "        acc = np.random.normal(1 if stage == \"Awake\" else 0.5 if stage == \"Light\" else 0.2 if stage == \"Deep\" else 0.3, 0.2, samples_per_stage)\n",
    "        \n",
    "        # Introduce random variations to simulate real-world sensor noise\n",
    "        transition_points = np.random.choice(samples_per_stage - 5, int(0.1 * samples_per_stage), replace=False)\n",
    "        for tp in transition_points:\n",
    "            hr[tp:tp + 5] += np.random.normal(5, 2, 5)\n",
    "            acc[tp:tp + 5] += np.random.normal(0.1, 0.05, 5)\n",
    "        \n",
    "        # Combine into a DataFrame\n",
    "        stage_data = pd.DataFrame({\n",
    "            \"heart_rate\": hr,\n",
    "            \"accelerometer\": acc,\n",
    "            \"stage\": [stage] * samples_per_stage\n",
    "        })\n",
    "        data.append(stage_data)\n",
    "\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Generate dataset\n",
    "data = generate_large_synthetic_data()\n",
    "\n",
    "# Display the entire dataset in a notebook environment\n",
    "##import ace_tools as tools; tools.display_dataframe_to_user(name=\"Generated Sleep Activity Dataset\", dataframe=data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e10b7243-c8a0-4ebb-8195-0ff437181ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (600, 3)\n",
      "     heart_rate  accelerometer  stage\n",
      "0     73.331580       1.059298  Awake\n",
      "1     68.253495       0.929562  Awake\n",
      "2     87.923379       0.943500  Awake\n",
      "3     86.926818       1.133956  Awake\n",
      "4     78.941390       0.848318  Awake\n",
      "5     85.371622       1.352253  Awake\n",
      "6     69.005387       1.157244  Awake\n",
      "7     68.661631       1.178648  Awake\n",
      "8    107.657456       1.535010  Awake\n",
      "9     66.705837       1.434847  Awake\n",
      "10    98.109032       0.880834  Awake\n",
      "11    72.368284       0.813972  Awake\n",
      "12    66.710617       1.098789  Awake\n",
      "13    53.014633       1.117695  Awake\n",
      "14    72.177611       0.933898  Awake\n",
      "15    88.237048       1.375299  Awake\n",
      "16    62.304552       0.947451  Awake\n",
      "17    81.669298       1.166856  Awake\n",
      "18    68.540471       0.977605  Awake\n",
      "19    68.106260       0.679478  Awake\n",
      "20    87.033141       1.079887  Awake\n",
      "21    52.684935       0.783834  Awake\n",
      "22    72.090304       1.359814  Awake\n",
      "23    64.975734       0.977000  Awake\n",
      "24    73.907128       0.858998  Awake\n",
      "25    63.038297       0.562750  Awake\n",
      "26    64.268601       0.938888  Awake\n",
      "27    79.544833       1.100265  Awake\n",
      "28    45.772277       0.719380  Awake\n",
      "29    76.885091       0.767880  Awake\n",
      "30    74.123180       1.170148  Awake\n",
      "31    78.875764       1.047218  Awake\n",
      "32    87.962804       1.045276  Awake\n",
      "33    65.955223       1.181387  Awake\n",
      "34    83.094951       1.232722  Awake\n",
      "35    83.197312       0.781275  Awake\n",
      "36    59.597819       0.903686  Awake\n",
      "37    67.455989       0.932938  Awake\n",
      "38    61.628298       0.830958  Awake\n",
      "39    66.204629       1.036119  Awake\n",
      "40    75.390383       0.599222  Awake\n",
      "41    75.520700       0.856041  Awake\n",
      "42    76.821693       1.275126  Awake\n",
      "43    81.971731       0.985226  Awake\n",
      "44    85.959623       1.190551  Awake\n",
      "45    73.092722       1.242869  Awake\n",
      "46    62.750205       1.508984  Awake\n",
      "47    76.125360       0.904480  Awake\n",
      "48    56.990633       0.579598  Awake\n",
      "49    74.516577       0.878717  Awake\n",
      "50    83.274749       1.028541  Awake\n",
      "51    54.023306       1.067093  Awake\n",
      "52    61.875017       1.184731  Awake\n",
      "53    81.874670       0.820421  Awake\n",
      "54    50.835009       0.671189  Awake\n",
      "55    78.637951       1.218986  Awake\n",
      "56    66.766254       1.222413  Awake\n",
      "57    73.960560       1.219009  Awake\n",
      "58    76.032060       1.559640  Awake\n",
      "59    94.445060       0.788063  Awake\n",
      "60    75.494393       1.276531  Awake\n",
      "61    70.761867       0.919168  Awake\n",
      "62    58.354986       1.483174  Awake\n",
      "63    59.940631       1.402206  Awake\n",
      "64    62.388104       1.308814  Awake\n",
      "65    73.373279       1.217004  Awake\n",
      "66    75.113448       1.125510  Awake\n",
      "67    36.778035       1.229147  Awake\n",
      "68    54.996249       0.801727  Awake\n",
      "69    66.873479       0.901029  Awake\n",
      "70    81.483556       0.752860  Awake\n",
      "71    65.395689       1.040915  Awake\n",
      "72    76.675142       0.849381  Awake\n",
      "73    68.329163       0.882627  Awake\n",
      "74    82.961574       1.150142  Awake\n",
      "75    55.441116       0.979873  Awake\n",
      "76    65.677557       1.084161  Awake\n",
      "77    73.581210       1.027859  Awake\n",
      "78    60.079917       1.046400  Awake\n",
      "79    62.350327       0.924937  Awake\n",
      "80    80.889285       1.402380  Awake\n",
      "81    56.369102       1.220420  Awake\n",
      "82    62.503905       1.255757  Awake\n",
      "83    84.755148       0.863259  Awake\n",
      "84    69.701712       0.877182  Awake\n",
      "85    71.835505       1.047379  Awake\n",
      "86    78.891019       0.855881  Awake\n",
      "87   104.058238       0.762223  Awake\n",
      "88    67.525546       1.542118  Awake\n",
      "89    63.303742       1.299209  Awake\n",
      "90    85.392026       1.357999  Awake\n",
      "91    65.997455       1.156655  Awake\n",
      "92    69.473901       1.196021  Awake\n",
      "93    71.764088       1.167862  Awake\n",
      "94    64.188080       1.316473  Awake\n",
      "95    75.865083       0.948428  Awake\n",
      "96    69.710165       1.087886  Awake\n",
      "97    54.923608       0.881942  Awake\n",
      "98    77.816331       0.757147  Awake\n",
      "99    68.077202       0.857320  Awake\n",
      "100   49.374964       1.036777  Awake\n",
      "101   63.899966       1.237766  Awake\n",
      "102   56.351343       1.379932  Awake\n",
      "103   76.250897       0.887329  Awake\n",
      "104   73.139858       0.988404  Awake\n",
      "105   70.561702       0.760024  Awake\n",
      "106   77.838616       1.188103  Awake\n",
      "107   69.493661       1.244354  Awake\n",
      "108   77.978929       1.331270  Awake\n",
      "109   95.275890       1.238125  Awake\n",
      "110   68.446573       1.217238  Awake\n",
      "111   84.813241       1.412501  Awake\n",
      "112   72.492298       1.180267  Awake\n",
      "113   58.826374       1.106122  Awake\n",
      "114   54.636625       0.773241  Awake\n",
      "115   74.764935       0.790187  Awake\n",
      "116   83.765713       1.187148  Awake\n",
      "117   66.646767       1.156970  Awake\n",
      "118   66.309829       1.226986  Awake\n",
      "119   74.760395       1.493470  Awake\n",
      "120   70.556539       0.948928  Awake\n",
      "121   56.429570       1.276276  Awake\n",
      "122   57.296244       0.771123  Awake\n",
      "123   68.416863       1.123391  Awake\n",
      "124   80.394586       1.042025  Awake\n",
      "125   82.537836       0.817235  Awake\n",
      "126   70.747700       0.977460  Awake\n",
      "127   69.846988       1.253280  Awake\n",
      "128   86.157443       0.606887  Awake\n",
      "129   91.045005       0.984519  Awake\n",
      "130   74.134116       0.819618  Awake\n",
      "131   71.390005       0.842120  Awake\n",
      "132   81.745788       0.873791  Awake\n",
      "133   66.988745       0.851271  Awake\n",
      "134   67.785955       0.739807  Awake\n",
      "135   67.256818       0.794492  Awake\n",
      "136   67.519534       0.863359  Awake\n",
      "137   54.747900       0.888555  Awake\n",
      "138   83.383485       0.788044  Awake\n",
      "139   61.061159       1.173351  Awake\n",
      "140   68.271463       0.704833  Awake\n",
      "141   84.410216       0.822710  Awake\n",
      "142   82.651552       1.134539  Awake\n",
      "143   71.782550       1.072772  Awake\n",
      "144   66.682958       0.988439  Awake\n",
      "145   71.141271       1.350804  Awake\n",
      "146   89.461368       0.905181  Awake\n",
      "147   71.034995       1.050110  Awake\n",
      "148   57.360438       1.064849  Awake\n",
      "149   74.813315       0.900489  Awake\n",
      "150   71.568280       0.276147  Light\n",
      "151   61.539697       0.130993  Light\n",
      "152   67.396170       0.424788  Light\n",
      "153   59.291923       0.503592  Light\n",
      "154   52.937302       0.427543  Light\n",
      "155   77.515510       0.287699  Light\n",
      "156   69.891836       0.532282  Light\n",
      "157   73.920979       0.499962  Light\n",
      "158   48.722689       0.603606  Light\n",
      "159   56.488236       0.485987  Light\n",
      "160   56.760301       0.468937  Light\n",
      "161   56.938869       0.473844  Light\n",
      "162   77.385754       0.736201  Light\n",
      "163   69.582734       0.470852  Light\n",
      "164   74.553280       0.834979  Light\n",
      "165   67.172958       0.413266  Light\n",
      "166   61.037210       0.845702  Light\n",
      "167   75.469351       0.264995  Light\n",
      "168   43.541676       0.341153  Light\n",
      "169   56.001054       0.485662  Light\n",
      "170   51.115040       0.237157  Light\n",
      "171   69.607074       0.624984  Light\n",
      "172   57.677533       0.610484  Light\n",
      "173   64.731470       0.409529  Light\n",
      "174   62.111701       0.853612  Light\n",
      "175   67.399039       0.367610  Light\n",
      "176   68.027849       0.755023  Light\n",
      "177   84.888173       0.681157  Light\n",
      "178   42.276356       1.243178  Light\n",
      "179   57.987023       0.493197  Light\n",
      "180   77.092970       0.747291  Light\n",
      "181   71.438581       0.755453  Light\n",
      "182   58.076501       0.899493  Light\n",
      "183   65.270615       1.157756  Light\n",
      "184  102.065312       0.323137  Light\n",
      "185   80.975599       0.915791  Light\n",
      "186   76.118910       0.667335  Light\n",
      "187   46.040300       0.328131  Light\n",
      "188   59.113576       0.159427  Light\n",
      "189   63.246237       0.151691  Light\n",
      "190   59.106093       0.395633  Light\n",
      "191   71.184248       0.497061  Light\n",
      "192   59.966617       0.285256  Light\n",
      "193   66.247731       0.671954  Light\n",
      "194   47.411181       0.823759  Light\n",
      "195   58.284735       0.336051  Light\n",
      "196   59.723629       0.321909  Light\n",
      "197   39.252281       0.120033  Light\n",
      "198   61.935101       0.049977  Light\n",
      "199   50.026997       0.549302  Light\n",
      "200   53.664327       0.635063  Light\n",
      "201   66.236229       0.662991  Light\n",
      "202   71.937007       0.677064  Light\n",
      "203   68.392643       0.869604  Light\n",
      "204   85.481191       0.627247  Light\n",
      "205   58.186630       0.632879  Light\n",
      "206   69.857680       0.856394  Light\n",
      "207   76.778577       0.441297  Light\n",
      "208   54.280546       0.700219  Light\n",
      "209   57.663349       0.856880  Light\n",
      "210   72.975430       0.377744  Light\n",
      "211   65.742070       0.467921  Light\n",
      "212   70.938243       0.703557  Light\n",
      "213   41.299863       0.669129  Light\n",
      "214   64.214518       0.674765  Light\n",
      "215   56.581945       0.299236  Light\n",
      "216   57.304081       0.702796  Light\n",
      "217   56.968562       0.797005  Light\n",
      "218   58.533291       0.624031  Light\n",
      "219   56.388474       0.769296  Light\n",
      "220   51.262149       0.560036  Light\n",
      "221   79.326667       0.430128  Light\n",
      "222   76.100993       0.293561  Light\n",
      "223   63.078986       0.745195  Light\n",
      "224   90.842659       0.545437  Light\n",
      "225   43.181232       0.696245  Light\n",
      "226   79.024761       0.532425  Light\n",
      "227   62.187280       0.548946  Light\n",
      "228   71.486280       0.693012  Light\n",
      "229   47.529037       0.742620  Light\n",
      "230   67.185059       0.480909  Light\n",
      "231   43.279169       0.487131  Light\n",
      "232   62.883705       0.418128  Light\n",
      "233   54.330313       0.633875  Light\n",
      "234   47.844764       0.559882  Light\n",
      "235   61.277163       0.571807  Light\n",
      "236   60.170436       0.559520  Light\n",
      "237   53.074582       0.465034  Light\n",
      "238   64.682491       0.268763  Light\n",
      "239   82.963402       0.532545  Light\n",
      "240   72.065990       0.843410  Light\n",
      "241   84.535660       0.529857  Light\n",
      "242   52.503057       0.554547  Light\n",
      "243   57.719982       0.291685  Light\n",
      "244   67.884138       0.427687  Light\n",
      "245   69.074489       0.492609  Light\n",
      "246   40.358819       0.446167  Light\n",
      "247   50.556044       0.800739  Light\n",
      "248   47.152668       0.563385  Light\n",
      "249   37.198901       0.482426  Light\n",
      "250   49.168718       0.265160  Light\n",
      "251   62.280870       0.139956  Light\n",
      "252   54.300280       0.564195  Light\n",
      "253   42.615297       0.764163  Light\n",
      "254   54.556620       0.502697  Light\n",
      "255   68.438393       0.655437  Light\n",
      "256   50.471928       0.607097  Light\n",
      "257   63.125324       0.439492  Light\n",
      "258   74.165109       0.701712  Light\n",
      "259   71.339607       0.266665  Light\n",
      "260   70.079999       0.622689  Light\n",
      "261   46.028141       0.379371  Light\n",
      "262   66.170157       0.369398  Light\n",
      "263   52.149569       0.744085  Light\n",
      "264   71.393250       0.291445  Light\n",
      "265   59.534753       0.520410  Light\n",
      "266   75.624928       0.288039  Light\n",
      "267   59.748030       0.570976  Light\n",
      "268   55.898962       0.666711  Light\n",
      "269   63.775693       0.374103  Light\n",
      "270   64.008918       0.484253  Light\n",
      "271   61.424957       0.461765  Light\n",
      "272   59.075721       0.436001  Light\n",
      "273   53.021207       0.927304  Light\n",
      "274   68.436050       0.404244  Light\n",
      "275   74.919682       0.572395  Light\n",
      "276   55.503462       0.351964  Light\n",
      "277   47.933438       0.392796  Light\n",
      "278   53.226000       0.587207  Light\n",
      "279   47.374490       0.401754  Light\n",
      "280   49.478295       0.492334  Light\n",
      "281   63.226984       0.748140  Light\n",
      "282   80.391304       0.546775  Light\n",
      "283   59.176527       0.709508  Light\n",
      "284   61.865333       0.528241  Light\n",
      "285   63.532974       0.585344  Light\n",
      "286   55.916257       0.612018  Light\n",
      "287   59.142917       0.530793  Light\n",
      "288   59.118916       0.365001  Light\n",
      "289   68.237504       0.735611  Light\n",
      "290   82.703611       0.344280  Light\n",
      "291   50.797240       0.443954  Light\n",
      "292   58.283510       0.528152  Light\n",
      "293   56.401208       0.332323  Light\n",
      "294   65.961068       0.564522  Light\n",
      "295   42.562122       0.533568  Light\n",
      "296   67.748188       0.482450  Light\n",
      "297   62.281190       0.504017  Light\n",
      "298   65.479828       0.384879  Light\n",
      "299   58.702727       0.795575  Light\n",
      "300   48.416013       0.061349   Deep\n",
      "301   66.612427       0.211499   Deep\n",
      "302   37.251190      -0.006837   Deep\n",
      "303   67.271845       0.224666   Deep\n",
      "304   64.983091      -0.069681   Deep\n",
      "305   46.803381       0.506947   Deep\n",
      "306   60.515192       0.187897   Deep\n",
      "307   52.458705       0.004204   Deep\n",
      "308   39.163295       0.087548   Deep\n",
      "309   29.085531      -0.123313   Deep\n",
      "310   36.592559       0.148932   Deep\n",
      "311   32.531423       0.256815   Deep\n",
      "312   54.106275       0.139866   Deep\n",
      "313   53.473552       0.039666   Deep\n",
      "314   69.051683       0.072262   Deep\n",
      "315   51.777001       0.293315   Deep\n",
      "316   57.319760       0.095900   Deep\n",
      "317   50.211697       0.492366   Deep\n",
      "318   65.775890       0.724091   Deep\n",
      "319   58.390255       0.390249   Deep\n",
      "320   60.048845       0.682746   Deep\n",
      "321   56.023241       0.442137   Deep\n",
      "322   62.047349       0.488351   Deep\n",
      "323   63.709432       0.133682   Deep\n",
      "324   43.378797       0.281117   Deep\n",
      "325   63.372931       0.326165   Deep\n",
      "326   59.733495       0.088464   Deep\n",
      "327   65.955484       0.245024   Deep\n",
      "328   46.118147       0.348268   Deep\n",
      "329   64.324652       0.288580   Deep\n",
      "330   49.599066       0.188993   Deep\n",
      "331   61.217472       0.158315   Deep\n",
      "332   62.562884       0.407055   Deep\n",
      "333   67.340731       0.396495   Deep\n",
      "334   39.884168       0.203592   Deep\n",
      "335   40.044279       0.314521   Deep\n",
      "336   56.661073       0.310726   Deep\n",
      "337   48.780223       0.288938   Deep\n",
      "338   43.052941       0.171872   Deep\n",
      "339   38.061757      -0.012655   Deep\n",
      "340   51.586528      -0.124465   Deep\n",
      "341   36.143138       0.398571   Deep\n",
      "342   40.429349      -0.022292   Deep\n",
      "343   37.761475       0.061020   Deep\n",
      "344   54.191472       0.282527   Deep\n",
      "345   41.561276       0.114918   Deep\n",
      "346   59.736904       0.297219   Deep\n",
      "347   46.978744       0.381979   Deep\n",
      "348   44.921196       0.538034   Deep\n",
      "349   47.017323      -0.170438   Deep\n",
      "350   59.369701       0.488341   Deep\n",
      "351   46.510049       0.236211   Deep\n",
      "352   43.111092       0.071604   Deep\n",
      "353   54.232621       0.342540   Deep\n",
      "354   57.452492       0.741770   Deep\n",
      "355   51.237783       0.375282   Deep\n",
      "356   55.785556       0.311819   Deep\n",
      "357   48.295441       0.430135   Deep\n",
      "358   50.693645       0.405527   Deep\n",
      "359   53.848045       0.170814   Deep\n",
      "360   34.150732       0.390682   Deep\n",
      "361   47.141487       0.572218   Deep\n",
      "362   55.832262       0.036961   Deep\n",
      "363   41.083266       0.205812   Deep\n",
      "364   55.315357       0.076917   Deep\n",
      "365   53.098413       0.598804   Deep\n",
      "366   75.558458       0.683566   Deep\n",
      "367   68.154337       0.492048   Deep\n",
      "368   68.466293       0.658959   Deep\n",
      "369   46.389132       0.164292   Deep\n",
      "370   48.889463       0.218924   Deep\n",
      "371   52.256219       0.403009   Deep\n",
      "372   49.376719       0.110341   Deep\n",
      "373   48.399507       0.247065   Deep\n",
      "374   42.734536       0.279545   Deep\n",
      "375   49.641159       0.275591   Deep\n",
      "376   46.169415       0.071470   Deep\n",
      "377   48.233702      -0.015619   Deep\n",
      "378   46.078508       0.267005   Deep\n",
      "379   38.283539       0.170712   Deep\n",
      "380   58.027835       0.219277   Deep\n",
      "381   68.577292       0.571523   Deep\n",
      "382   37.912520      -0.033519   Deep\n",
      "383   70.396787       0.715052   Deep\n",
      "384   44.562942       0.089236   Deep\n",
      "385   39.937200       0.259453   Deep\n",
      "386   35.111333       0.031211   Deep\n",
      "387   22.140880       0.115117   Deep\n",
      "388   57.514579       0.154693   Deep\n",
      "389   33.092584       0.559121   Deep\n",
      "390   48.511908      -0.127438   Deep\n",
      "391   63.987234       0.214128   Deep\n",
      "392   56.078336       0.389053   Deep\n",
      "393   47.533476       0.218134   Deep\n",
      "394   44.838452       0.165532   Deep\n",
      "395   38.869073       0.251249   Deep\n",
      "396   35.025001       0.309828   Deep\n",
      "397   59.344356       0.349144   Deep\n",
      "398   69.013349       0.203008   Deep\n",
      "399   59.926392       0.345300   Deep\n",
      "400   49.412719       0.533176   Deep\n",
      "401   47.219605      -0.052776   Deep\n",
      "402   66.731845       0.146109   Deep\n",
      "403   54.529955       0.759011   Deep\n",
      "404   63.323831       0.230363   Deep\n",
      "405   73.910903       0.237369   Deep\n",
      "406   65.583165      -0.071693   Deep\n",
      "407   51.153312       0.135349   Deep\n",
      "408   54.880832      -0.001466   Deep\n",
      "409   60.495450       0.172234   Deep\n",
      "410   52.881663       0.138214   Deep\n",
      "411   47.715675      -0.050862   Deep\n",
      "412   45.127377       0.217973   Deep\n",
      "413   67.758471       0.119670   Deep\n",
      "414   51.656679       0.226950   Deep\n",
      "415   48.045482       0.063527   Deep\n",
      "416   53.255552       0.823799   Deep\n",
      "417   59.206069       0.204293   Deep\n",
      "418   45.029082       0.490026   Deep\n",
      "419   62.590649       0.251344   Deep\n",
      "420   66.511658       0.192021   Deep\n",
      "421   48.558055      -0.078224   Deep\n",
      "422   48.458101       0.235027   Deep\n",
      "423   40.538247       0.078321   Deep\n",
      "424   75.255955       0.281804   Deep\n",
      "425   53.743108       0.112910   Deep\n",
      "426   44.979378       0.128641   Deep\n",
      "427   37.531099       0.355781   Deep\n",
      "428   47.250542       0.143886   Deep\n",
      "429   44.585697       0.123345   Deep\n",
      "430   60.158137       0.638931   Deep\n",
      "431   66.356782       0.479073   Deep\n",
      "432   66.871917       0.289995   Deep\n",
      "433   69.233427       0.322350   Deep\n",
      "434   61.783381       0.490529   Deep\n",
      "435   50.209126       0.399080   Deep\n",
      "436   49.161709       0.292427   Deep\n",
      "437   57.892212       0.542976   Deep\n",
      "438   60.729915       0.060864   Deep\n",
      "439   43.766744       0.458758   Deep\n",
      "440   33.749084       0.606567   Deep\n",
      "441   38.688336       0.278610   Deep\n",
      "442   57.903903       0.705633   Deep\n",
      "443   67.038812      -0.370529   Deep\n",
      "444   49.774719       0.206007   Deep\n",
      "445   53.053136      -0.065683   Deep\n",
      "446   56.546314       0.181607   Deep\n",
      "447   64.369711       0.594012   Deep\n",
      "448   68.326919       0.266795   Deep\n",
      "449   42.720260       0.639026   Deep\n",
      "450   67.800210       0.351136    REM\n",
      "451   58.178070       0.582582    REM\n",
      "452   57.988044       0.159802    REM\n",
      "453   52.548571       0.595178    REM\n",
      "454   71.079745       0.205739    REM\n",
      "455   70.100121       0.430271    REM\n",
      "456   73.482421       0.243159    REM\n",
      "457   59.113825       0.391094    REM\n",
      "458   69.879507       0.227208    REM\n",
      "459   81.510317       0.182829    REM\n",
      "460   69.932971      -0.069119    REM\n",
      "461   82.724153       0.453828    REM\n",
      "462   75.536191       0.659031    REM\n",
      "463   72.396962       0.112743    REM\n",
      "464   69.308575       0.017515    REM\n",
      "465   68.807090       0.290050    REM\n",
      "466   72.308236       0.400497    REM\n",
      "467   71.793540       0.230906    REM\n",
      "468   76.239538       0.696968    REM\n",
      "469   63.686927      -0.111532    REM\n",
      "470   62.301187       0.152251    REM\n",
      "471   51.503192       0.448468    REM\n",
      "472   66.992406       0.603151    REM\n",
      "473   58.625227       0.547345    REM\n",
      "474   74.267887       0.249659    REM\n",
      "475   68.619828       0.216856    REM\n",
      "476   74.528125       0.228698    REM\n",
      "477   62.226785       0.313500    REM\n",
      "478   70.118567       0.549360    REM\n",
      "479   62.760016       0.641929    REM\n",
      "480   76.745805       0.320306    REM\n",
      "481   67.473518       0.099967    REM\n",
      "482   54.151242       0.039232    REM\n",
      "483   68.592600       0.362107    REM\n",
      "484   79.734820       0.332531    REM\n",
      "485   61.453990       0.535890    REM\n",
      "486   76.674019       0.103251    REM\n",
      "487   73.104839       0.094357    REM\n",
      "488   64.383751       0.273777    REM\n",
      "489   55.014418       0.801278    REM\n",
      "490   75.772102       0.261617    REM\n",
      "491   78.727118       0.471097    REM\n",
      "492   70.752651       0.981779    REM\n",
      "493   80.225334       0.602942    REM\n",
      "494   66.566731       0.405324    REM\n",
      "495   61.588591       0.460284    REM\n",
      "496   44.264322       0.103592    REM\n",
      "497   67.868151       0.352490    REM\n",
      "498   41.443468       0.442282    REM\n",
      "499   64.958338       0.526588    REM\n",
      "500   76.812315       0.322678    REM\n",
      "501   68.713996       0.351318    REM\n",
      "502   61.569798       0.563199    REM\n",
      "503   70.017436       0.420992    REM\n",
      "504   58.557871       0.385341    REM\n",
      "505   78.955359       0.023390    REM\n",
      "506   63.627434       0.670357    REM\n",
      "507   62.934752       0.476837    REM\n",
      "508   63.749297       0.667191    REM\n",
      "509   70.069440       0.435142    REM\n",
      "510   57.345142       0.398674    REM\n",
      "511   62.982014       0.178788    REM\n",
      "512   71.955632       0.504110    REM\n",
      "513   64.637055       0.404054    REM\n",
      "514   58.173447       0.305712    REM\n",
      "515   54.434599       0.194614    REM\n",
      "516   78.838436       0.575582    REM\n",
      "517   67.764598       0.185356    REM\n",
      "518   70.602663       0.510484    REM\n",
      "519   69.784175       0.850574    REM\n",
      "520   67.921143       0.240657    REM\n",
      "521   86.012853       0.353193    REM\n",
      "522   73.285487       0.391379    REM\n",
      "523   86.919383       0.430411    REM\n",
      "524   79.262713       0.323465    REM\n",
      "525   88.355974       0.511864    REM\n",
      "526   69.955334       0.265971    REM\n",
      "527   74.282420       0.652078    REM\n",
      "528   60.742056       0.350178    REM\n",
      "529   79.277165       0.435685    REM\n",
      "530   55.648467       0.309660    REM\n",
      "531   72.622600       0.354384    REM\n",
      "532   50.644465       0.317862    REM\n",
      "533   50.938723       0.294552    REM\n",
      "534   70.278469       0.766239    REM\n",
      "535   88.991143       0.363519    REM\n",
      "536   82.640247       0.196209    REM\n",
      "537   87.454876       0.118022    REM\n",
      "538   80.045016       0.911818    REM\n",
      "539   69.750006       0.262455    REM\n",
      "540   67.524930       0.573467    REM\n",
      "541   84.573189       0.366064    REM\n",
      "542   80.491709       0.042646    REM\n",
      "543   57.169259       0.660014    REM\n",
      "544   65.611377       0.482056    REM\n",
      "545   45.548499       0.649255    REM\n",
      "546   75.415114       0.198565    REM\n",
      "547   56.251374       0.281450    REM\n",
      "548   79.127957       0.441645    REM\n",
      "549   64.864728       0.059339    REM\n",
      "550   64.236037       0.603114    REM\n",
      "551   67.856419       0.176804    REM\n",
      "552   69.340917       0.476960    REM\n",
      "553   64.591632       0.519347    REM\n",
      "554   83.238537       0.235077    REM\n",
      "555   64.263738       0.427869    REM\n",
      "556   64.928544       0.365191    REM\n",
      "557   69.899371       0.347506    REM\n",
      "558   65.096988       0.707522    REM\n",
      "559   57.225770       0.249608    REM\n",
      "560   50.170049       0.616589    REM\n",
      "561   89.976069       0.366197    REM\n",
      "562   60.403804       0.495646    REM\n",
      "563   67.616980       0.464695    REM\n",
      "564   77.172903       0.599727    REM\n",
      "565   59.675023       0.586721    REM\n",
      "566   65.206140      -0.167200    REM\n",
      "567   64.744971       0.199596    REM\n",
      "568   63.974952       0.519225    REM\n",
      "569   73.154364       0.414638    REM\n",
      "570   70.020201       0.280566    REM\n",
      "571   65.577260       0.265705    REM\n",
      "572   51.959949       0.205417    REM\n",
      "573   73.417656       0.375868    REM\n",
      "574   59.406195       0.031972    REM\n",
      "575   83.592307       0.283343    REM\n",
      "576   48.011268       0.120037    REM\n",
      "577   63.265041       0.427392    REM\n",
      "578   64.171903       0.296423    REM\n",
      "579   74.794014       0.478338    REM\n",
      "580   67.882352       0.573559    REM\n",
      "581   62.320409       0.439842    REM\n",
      "582   64.585926       0.585619    REM\n",
      "583   82.862057      -0.048403    REM\n",
      "584   78.784916       0.537703    REM\n",
      "585   61.015610       0.560682    REM\n",
      "586   76.913772       0.216091    REM\n",
      "587   55.334130       0.211491    REM\n",
      "588   62.692134       0.495546    REM\n",
      "589   76.510161       0.138954    REM\n",
      "590   73.658208       0.401583    REM\n",
      "591   73.410074       0.403485    REM\n",
      "592   65.182014       0.258017    REM\n",
      "593   39.987460       0.603567    REM\n",
      "594   65.198171       0.374093    REM\n",
      "595   79.622900       0.673960    REM\n",
      "596   63.623576       0.426583    REM\n",
      "597   43.504618       0.146932    REM\n",
      "598   57.867257       0.139782    REM\n",
      "599   64.093646       0.173680    REM\n"
     ]
    }
   ],
   "source": [
    "# Display the full dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure all rows and columns are visible in the notebook\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the dataset\n",
    "print(\"Dataset Shape:\", data.shape)  # Print the shape of the dataset\n",
    "print(data)  # Print the dataset content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1381d8a0-ff1d-4f69-9d46-f951e7dd20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Preprocess the Dataset\n",
    "def preprocess_data(data, sequence_length=50):\n",
    "    # Encode the labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['stage'] = label_encoder.fit_transform(data['stage'])\n",
    "    unique_classes = label_encoder.classes_\n",
    "\n",
    "    # Extract features and labels\n",
    "    X = data[['heart_rate', 'accelerometer']].values\n",
    "    y = data['stage'].values\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X_scaled) - sequence_length):\n",
    "        X_seq.append(X_scaled[i:i + sequence_length])\n",
    "        y_seq.append(y[i + sequence_length])\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq), scaler, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f7bfba-fcff-4258-aa34-40d79ac8f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (385, 50, 2), Validation data shape: (82, 50, 2), Testing data shape: (83, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the generated dataset\n",
    "sequence_length = 50\n",
    "X_seq, y_seq, scaler, label_encoder = preprocess_data(data, sequence_length)\n",
    "\n",
    "# Step 2: Split the Dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Validation data shape: {X_val.shape}, Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52fa205b-2e52-4a6a-bf4f-66bedd394a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),  # Use Input as the first layer\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(5, activation='softmax')  # 5 sleep stages\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c980bab-27e5-4342-b52c-ca793347361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - accuracy: 0.3026 - loss: 1.5826 - val_accuracy: 0.6627 - val_loss: 1.3731\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6487 - loss: 1.3268 - val_accuracy: 0.7229 - val_loss: 0.9875\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6686 - loss: 0.9690 - val_accuracy: 0.9277 - val_loss: 0.7130\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9338 - loss: 0.6655 - val_accuracy: 0.9639 - val_loss: 0.4508\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9236 - loss: 0.4918 - val_accuracy: 0.9639 - val_loss: 0.3316\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9369 - loss: 0.3490 - val_accuracy: 0.9639 - val_loss: 0.2686\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9434 - loss: 0.3039 - val_accuracy: 0.9639 - val_loss: 0.2207\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9522 - loss: 0.2433 - val_accuracy: 0.9759 - val_loss: 0.1923\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9634 - loss: 0.2036 - val_accuracy: 0.9639 - val_loss: 0.1943\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9673 - loss: 0.1593 - val_accuracy: 0.9639 - val_loss: 0.1665\n",
      "Epoch 1: Loss = 1.5418, Accuracy = 0.4935, Validation Loss = 1.3731, Validation Accuracy = 0.6627\n",
      "Epoch 2: Loss = 1.2431, Accuracy = 0.6597, Validation Loss = 0.9875, Validation Accuracy = 0.7229\n",
      "Epoch 3: Loss = 0.9235, Accuracy = 0.7039, Validation Loss = 0.7130, Validation Accuracy = 0.9277\n",
      "Epoch 4: Loss = 0.6484, Accuracy = 0.9273, Validation Loss = 0.4508, Validation Accuracy = 0.9639\n",
      "Epoch 5: Loss = 0.4407, Accuracy = 0.9403, Validation Loss = 0.3316, Validation Accuracy = 0.9639\n",
      "Epoch 6: Loss = 0.3384, Accuracy = 0.9351, Validation Loss = 0.2686, Validation Accuracy = 0.9639\n",
      "Epoch 7: Loss = 0.2852, Accuracy = 0.9429, Validation Loss = 0.2207, Validation Accuracy = 0.9639\n",
      "Epoch 8: Loss = 0.2235, Accuracy = 0.9662, Validation Loss = 0.1923, Validation Accuracy = 0.9759\n",
      "Epoch 9: Loss = 0.2283, Accuracy = 0.9506, Validation Loss = 0.1943, Validation Accuracy = 0.9639\n",
      "Epoch 10: Loss = 0.1610, Accuracy = 0.9662, Validation Loss = 0.1665, Validation Accuracy = 0.9639\n",
      "Model and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Build and train the LSTM model\n",
    "model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Display validation accuracy during training\n",
    "for epoch, (loss, accuracy, val_loss, val_accuracy) in enumerate(zip(\n",
    "        history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy'])):\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}, Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "# Save model in Keras format and scaler\n",
    "model.save(\"sleep_lstm_model_combined.keras\")\n",
    "np.save(\"scaler_mean.npy\", scaler.mean_)\n",
    "np.save(\"scaler_scale.npy\", scaler.scale_)\n",
    "\n",
    "print(\"Model and scaler saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b53151-9c99-4285-9f1a-052b61e6db24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model on the test set...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9663 - loss: 0.1679\n",
      "Test Loss: 0.1665, Test Accuracy: 0.9639\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        20\n",
      "           1       0.95      0.95      0.95        22\n",
      "           2       0.95      0.95      0.95        20\n",
      "           3       1.00      0.95      0.98        21\n",
      "\n",
      "    accuracy                           0.96        83\n",
      "   macro avg       0.96      0.96      0.96        83\n",
      "weighted avg       0.96      0.96      0.96        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nEvaluating the model on the test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert one-hot probabilities to class indices\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b93875-b3e2-4505-b82c-70efc927f42c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'converter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Enable TensorFlow Ops for unsupported operations\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m converter\u001b[38;5;241m.\u001b[39mtarget_spec\u001b[38;5;241m.\u001b[39msupported_ops \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOpsSet\u001b[38;5;241m.\u001b[39mTFLITE_BUILTINS,  \u001b[38;5;66;03m# Default TFLite operations\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOpsSet\u001b[38;5;241m.\u001b[39mSELECT_TF_OPS    \u001b[38;5;66;03m# TensorFlow operations\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Disable experimental lowering of tensor list operations\u001b[39;00m\n\u001b[0;32m     10\u001b[0m converter\u001b[38;5;241m.\u001b[39m_experimental_lower_tensor_list_ops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'converter' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable TensorFlow Ops for unsupported operations\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Default TFLite operations\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS    # TensorFlow operations\n",
    "]\n",
    "\n",
    "# Disable experimental lowering of tensor list operations\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convert the model\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "    with open(\"sleep_lstm_model_with_tf_ops.tflite\", \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Model converted with TensorFlow ops and saved as 'sleep_lstm_model_with_tf_ops.tflite'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during conversion: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa45b4f3-cc73-491a-867b-a7629a3c967b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: [[0.02760666 0.00528242 0.95371723 0.00883773 0.00455591]]\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"sleep_lstm_model_with_tf_ops.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test with dummy input (adjust shape to match your model)\n",
    "import numpy as np\n",
    "dummy_input = np.random.rand(1, 50, 2).astype(np.float32)  # Replace with actual input shape\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], dummy_input)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output\n",
    "output = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Model Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1068ef62-73f9-4a98-8d10-78910ec37cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "tflite_model_path = \"sleep_lstm_model_with_tf_ops.tflite\"  # Ensure the correct path is specified\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(tflite_model_path):\n",
    "    raise FileNotFoundError(f\"The specified TFLite model file does not exist: {tflite_model_path}\")\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "print(\"TFLite model loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2948c066-f2dc-4a19-9695-2f73716fe666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow.js model has been saved to: tfjs_model\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "tflite_model_path = \"sleep_lstm_model_with_tf_ops.tflite\"  # Path to your TFLite model\n",
    "tfjs_output_dir = \"tfjs_model\"  # Directory to save the TensorFlow.js model\n",
    "\n",
    "# Use TensorFlow.js converter to directly convert the TFLite model to a JavaScript format\n",
    "import os\n",
    "os.system(f\"tensorflowjs_converter --input_format=tflite --output_format=tfjs_graph_model {tflite_model_path} {tfjs_output_dir}\")\n",
    "\n",
    "print(f\"TensorFlow.js model has been saved to: {tfjs_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f608cd1-82cb-43eb-8629-2070f4b733b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflowjs in d:\\anaconda\\lib\\site-packages (3.18.0)\n",
      "Requirement already satisfied: tensorflow<3,>=2.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: six<2,>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (0.12.0)\n",
      "Requirement already satisfied: packaging~=20.9 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging~=20.9->tensorflowjs) (3.1.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in d:\\anaconda\\lib\\site-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (25.1.21)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (75.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.44.0)\n",
      "Requirement already satisfied: rich in d:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (13.7.1)\n",
      "Requirement already satisfied: namex in d:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflowjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a549d99a-17bd-4848-bdb1-612784df6f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflowjs_converter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tensorflowjs_converter \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39mversion\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflowjs_converter' is not defined"
     ]
    }
   ],
   "source": [
    "tensorflowjs_converter --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e31b94c-c4c1-4ed3-b36d-6c428da3aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the version of tensorflowjs_converter\n",
    "os.system(\"tensorflowjs_converter --version\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76c5924f-a346-480c-a5d1-71acf25f54f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflowjs\n",
      "Version: 3.18.0\n",
      "Summary: \n",
      "Home-page: https://js.tensorflow.org/\n",
      "Author: Google LLC\n",
      "Author-email: opensource@google.com\n",
      "License: Apache 2.0\n",
      "Location: D:\\Anaconda\\Lib\\site-packages\n",
      "Requires: packaging, six, tensorflow, tensorflow-hub\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflowjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcaf902-d504-4ac6-9ec7-a44dbbcc7b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['tensorflowjs_converter', '--version'], returncode=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run tensorflowjs_converter to check the version\n",
    "subprocess.run([\"tensorflowjs_converter\", \"--version\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99605c8-0661-4d1a-acc3-6963dcfdb066",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2234867930.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    where tensorflowjs_converter\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "where tensorflowjs_converter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789b878d-bf1c-4a49-98b8-1adfb26322f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Code: 1\n",
      "Standard Output: \n",
      "Error Output: 2025-01-26 14:20:30.401951: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-26 14:20:31.931666: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"D:\\Anaconda\\Scripts\\tensorflowjs_converter.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs import converters\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\converters\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs.converters.converter import convert\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\converters\\converter.py\", line 37, in <module>\n",
      "    from tensorflowjs.converters import tf_saved_model_conversion_v2\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py\", line 42, in <module>\n",
      "    import tensorflow_hub as hub\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflow_hub\\__init__.py\", line 88, in <module>\n",
      "    from tensorflow_hub.estimator import LatestModuleExporter\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflow_hub\\estimator.py\", line 62, in <module>\n",
      "    class LatestModuleExporter(tf.compat.v1.estimator.Exporter):\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py\", line 232, in _getattr\n",
      "    attr = getattr(self._tfmw_wrapped_module, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'tensorflow.compat.v1' has no attribute 'estimator'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"tensorflowjs_converter\", \"--version\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(\"Return Code:\", result.returncode)\n",
    "print(\"Standard Output:\", result.stdout)\n",
    "print(\"Error Output:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2d427c-b6d5-41fe-94f7-ab4e1d1d90d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflowjs in d:\\anaconda\\lib\\site-packages (3.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow<3,>=2.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: six<2,>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (0.12.0)\n",
      "Requirement already satisfied: packaging~=20.9 in d:\\anaconda\\lib\\site-packages (from tensorflowjs) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging~=20.9->tensorflowjs) (3.1.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in d:\\anaconda\\lib\\site-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (25.1.21)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (75.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.44.0)\n",
      "Requirement already satisfied: rich in d:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (13.7.1)\n",
      "Requirement already satisfied: namex in d:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\anaconda\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflowjs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02fe543e-8231-4d45-96d4-adc5fa803a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression (2112401995.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensorflowjs_converter --input_format=tflite --output_format=tfjs_graph_model sleep_lstm_model_with_tf_ops.tflite tfjs_model\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to expression\n"
     ]
    }
   ],
   "source": [
    "tensorflowjs_converter --input_format=tflite --output_format=tfjs_graph_model sleep_lstm_model_with_tf_ops.tflite tfjs_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fbaf2ab-b630-41d9-8a65-5c2ac045e851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['tensorflowjs_converter', '--input_format=tflite', '--output_format=tfjs_graph_model', 'sleep_lstm_model_with_tf_ops.tflite', 'tfjs_model'], returncode=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run the command using os.system\n",
    "os.system(\"tensorflowjs_converter --input_format=tflite --output_format=tfjs_graph_model sleep_lstm_model_with_tf_ops.tflite tfjs_model\")\n",
    "\n",
    "# Alternatively, using subprocess\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"tensorflowjs_converter\",\n",
    "                \"--input_format=tflite\",\n",
    "                \"--output_format=tfjs_graph_model\",\n",
    "                \"sleep_lstm_model_with_tf_ops.tflite\",\n",
    "                \"tfjs_model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52776779-e6cc-4b6a-8e60-8fa3ad0b2581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 5209-F365\n",
      "\n",
      " Directory of C:\\Users\\girid\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\girid\n",
      "\n",
      "01/25/2025  11:10 PM            60,272 sleep_lstm_model_with_tf_ops.tflite\n",
      "               1 File(s)         60,272 bytes\n",
      "               0 Dir(s)  123,316,383,744 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls -l sleep_lstm_model_with_tf_ops.tflite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96718b69-de36-4cf2-a9e6-cfaaa812b5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model is valid and ready for conversion.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "interpreter = tf.lite.Interpreter(model_path=\"sleep_lstm_model_with_tf_ops.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "print(\"TFLite model is valid and ready for conversion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb9ff18-b34a-47b6-a915-ef13f2fff891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.18.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: D:\\Anaconda\\Lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: tensorflowjs\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "154ccede-cd2f-4246-bdaf-3b2ac2521731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return Code: 1\n",
      "Standard Output: \n",
      "Error Output: 2025-01-27 01:19:57.555278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-27 01:19:59.173258: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"D:\\Anaconda\\Scripts\\tensorflowjs_converter.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs import converters\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\converters\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs.converters.converter import convert\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\converters\\converter.py\", line 37, in <module>\n",
      "    from tensorflowjs.converters import tf_saved_model_conversion_v2\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflowjs\\converters\\tf_saved_model_conversion_v2.py\", line 42, in <module>\n",
      "    import tensorflow_hub as hub\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflow_hub\\__init__.py\", line 88, in <module>\n",
      "    from tensorflow_hub.estimator import LatestModuleExporter\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflow_hub\\estimator.py\", line 62, in <module>\n",
      "    class LatestModuleExporter(tf.compat.v1.estimator.Exporter):\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py\", line 232, in _getattr\n",
      "    attr = getattr(self._tfmw_wrapped_module, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'tensorflow.compat.v1' has no attribute 'estimator'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"tensorflowjs_converter\", \"--input_format=tflite\", \"--output_format=tfjs_graph_model\", \"sleep_lstm_model_with_tf_ops.tflite\", \"tfjs_model\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"Return Code:\", result.returncode)\n",
    "print(\"Standard Output:\", result.stdout)\n",
    "print(\"Error Output:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c3b1f4-1078-441e-97c1-a57a531ab73f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (2056763468.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensorflowjs_converter --input_format=keras sleep_lstm_model_combined.keras tfjs_model\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "tensorflowjs_converter --input_format=keras sleep_lstm_model_combined.keras tfjs_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1db6e2-2f69-428e-b54f-71bc029c4ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
